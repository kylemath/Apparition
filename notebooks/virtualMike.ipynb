{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apparition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kylemath/Apparition/blob/master/notebooks/virtualMike.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVOIVLl1zWY6"
      },
      "source": [
        "## Apparition\n",
        "\n",
        "** Notebook for creation, training, and saving of puppet using pix2pix **\n",
        "\n",
        "Goal is to make Apparition using old youtube footage. Will have live actor filmed with webcam during show whose pose gets transformed into real time Apparition Poses shown to audience, with voice impressonation reading lines scripted from dialogue AI. \n",
        "\n",
        "\n",
        "*   Using https://github.com/datitran/face2face-demo\n",
        "*   youtube video here https://www.youtube.com/watch?v=DjbQaSPPsqA\n",
        "*   saved into .mp4 with https://www.onlinevideoconverter.com/youtube-converter \n",
        "\n",
        "Instructions:\n",
        "* Connect colab to google drive \n",
        "* save the .mp4 video into a folder, \n",
        "* download the facial landmark model from http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "* after uncompressing locally and saving to google drive \n",
        "\n",
        "Tactics:\n",
        "* Convert video into side by side images of video and face structure\n",
        "* Train model on these side by side images to predict DelClose from Face pose\n",
        "* Use live webam to get net face structure\n",
        "* Convert new face structure to Apparition pose with model in real time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfVyESCxTaiu"
      },
      "source": [
        "## Prepare Environment ##\n",
        "\n",
        "* Instal git repo\n",
        "\n",
        "* Install anaconda and setup to use\n",
        "\n",
        "* Create conda environment and activate it\n",
        "\n",
        "* Install some needed packages\n",
        ">   ISSUE - dlib install takes about 10 minutes to compile, find way to speed up?\n",
        "\n",
        "* Mount your google drive to load and save from\n",
        " > Note you have to follow the link here and get the activation code before proceeding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIUdaNJ8Y5mK",
        "outputId": "2b9b0a64-01a9-4bdb-a618-5367e61d22ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#mount google drive to load own images, follow link and get code and paste in below\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "!git clone https://github.com/datitran/face2face-demo.git --recursive\n",
        "# Clone the repo from Christopher Hesse's pix2pix TensorFlow implementation\n",
        "!git clone https://github.com/affinelayer/pix2pix-tensorflow.git\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Cloning into 'face2face-demo'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Total 33 (delta 0), reused 0 (delta 0), pack-reused 33\u001b[K\n",
            "Unpacking objects: 100% (33/33), done.\n",
            "Cloning into 'pix2pix-tensorflow'...\n",
            "remote: Enumerating objects: 261, done.\u001b[K\n",
            "remote: Total 261 (delta 0), reused 0 (delta 0), pack-reused 261\u001b[K\n",
            "Receiving objects: 100% (261/261), 13.33 MiB | 37.71 MiB/s, done.\n",
            "Resolving deltas: 100% (103/103), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REuKAVgQSfq4",
        "outputId": "af4184d0-58f2-4d23-c02b-d6d06909008a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        }
      },
      "source": [
        "!wget -c https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh\n",
        "!chmod +x Anaconda3-5.1.0-Linux-x86_64.sh\n",
        "!bash ./Anaconda3-5.1.0-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
        "\n",
        "!conda env create -f face2face-demo/environment.yml \n",
        "#not sure this actually works in colab:\n",
        "!source activate face2face-demo\n",
        "#since I have to do this after:\n",
        "!pip install opencv-python\n",
        "!pip install imutils==0.4.3\n",
        "!pip install tensorflow==1.2.1\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-28 01:15:13--  https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.200.79, 104.18.201.79, 2606:4700::6812:c94f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.200.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/archive/Anaconda3-5.1.0-Linux-x86_64.sh [following]\n",
            "--2020-10-28 01:15:13--  https://repo.anaconda.com/archive/Anaconda3-5.1.0-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 577996269 (551M) [application/x-sh]\n",
            "Saving to: ‘Anaconda3-5.1.0-Linux-x86_64.sh’\n",
            "\n",
            "Anaconda3-5.1.0-Lin 100%[===================>] 551.22M   199MB/s    in 2.8s    \n",
            "\n",
            "2020-10-28 01:15:16 (199 MB/s) - ‘Anaconda3-5.1.0-Linux-x86_64.sh’ saved [577996269/577996269]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "installing: python-3.6.4-hc3d631a_1 ...\n",
            "Python 3.6.4 :: Anaconda, Inc.\n",
            "installing: ca-certificates-2017.08.26-h1d4fec5_0 ...\n",
            "installing: conda-env-2.6.0-h36134e3_1 ...\n",
            "installing: intel-openmp-2018.0.0-hc7b2577_8 ...\n",
            "installing: libgcc-ng-7.2.0-h7cc24e2_2 ...\n",
            "installing: libgfortran-ng-7.2.0-h9f7466a_2 ...\n",
            "installing: libstdcxx-ng-7.2.0-h7a57d05_2 ...\n",
            "installing: bzip2-1.0.6-h9a117a8_4 ...\n",
            "installing: expat-2.2.5-he0dffb1_0 ...\n",
            "installing: gmp-6.1.2-h6c8ec71_1 ...\n",
            "installing: graphite2-1.3.10-hf63cedd_1 ...\n",
            "installing: icu-58.2-h9c2bf20_1 ...\n",
            "installing: jbig-2.1-hdba287a_0 ...\n",
            "installing: jpeg-9b-h024ee3a_2 ...\n",
            "installing: libffi-3.2.1-hd88cf55_4 ...\n",
            "installing: libsodium-1.0.15-hf101ebd_0 ...\n",
            "installing: libtool-2.4.6-h544aabb_3 ...\n",
            "installing: libxcb-1.12-hcd93eb1_4 ...\n",
            "installing: lzo-2.10-h49e0be7_2 ...\n",
            "installing: mkl-2018.0.1-h19d6760_4 ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3QAfnNNa_4m"
      },
      "source": [
        "# only if doing stimulus generation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXYqBM2fY6lL"
      },
      "source": [
        "!pip install dlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xT6RqIhVTKe"
      },
      "source": [
        "# Processes the source video \n",
        "to find frames with faces, and pull out the face structure, save side by side image "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOW-THnuAOFh",
        "outputId": "13c480c0-8516-4f57-e0b9-ac9d6157710e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "print('Generating faces from video')\n",
        "!python face2face-demo/generate_train_data.py \\\n",
        "  --file /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/DelCloseMono.mp4 \\\n",
        "  --num 4000 \\\n",
        "  --landmark-model /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/shape_predictor_68_face_landmarks.dat\n",
        "\n",
        "print('Moving Images to Google Drive')\n",
        "# Move the original and landmarks folder into the google drive folder\n",
        "!mkdir /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/photos\n",
        "!cp -r landmarks -r original /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/photos\n",
        "\n",
        "print('Resizing Images')\n",
        "# Resize original images\n",
        "!python pix2pix-tensorflow/tools/process.py \\\n",
        "  --input_dir /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/photos/original \\\n",
        "  --operation resize \\\n",
        "  --output_dir /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/photos/original_resized\n",
        "  \n",
        "# Resize landmark images\n",
        "!python pix2pix-tensorflow/tools/process.py \\\n",
        "  --input_dir /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/photos/landmarks \\\n",
        "  --operation resize \\\n",
        "  --output_dir /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/photos/landmarks_resized\n",
        "\n",
        "print('Combining Images')\n",
        "# Combine both resized original and landmark images\n",
        "!python pix2pix-tensorflow/tools/process.py \\\n",
        "  --input_dir /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/photos/landmarks_resized \\\n",
        "  --b_dir /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/photos/original_resized \\\n",
        "  --operation combine \\\n",
        "  --output_dir /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/photos/combined\n",
        "  \n",
        "print('Spliting into training and val')\n",
        "# Split into train/val set\n",
        "!python pix2pix-tensorflow/tools/split.py \\\n",
        "  --dir /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/photos/combined"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating faces from video\n",
            "Traceback (most recent call last):\n",
            "  File \"face2face-demo/generate_train_data.py\", line 96, in <module>\n",
            "    predictor = dlib.shape_predictor(args.face_landmark_shape_file)\n",
            "KeyboardInterrupt\n",
            "Moving Images to Google Drive\n",
            "mkdir: cannot create directory ‘/content/gdrive/My Drive/DeepLearning/DelCloseBot/photos’: File exists\n",
            "Resizing Images\n",
            "python: can't open file 'pix2pix-tensorflow/tools/process.py': [Errno 2] No such file or directory\n",
            "python: can't open file 'pix2pix-tensorflow/tools/process.py': [Errno 2] No such file or directory\n",
            "Combining Images\n",
            "python: can't open file 'pix2pix-tensorflow/tools/process.py': [Errno 2] No such file or directory\n",
            "Spliting into training and val\n",
            "python: can't open file 'pix2pix-tensorflow/tools/split.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFKE2AHVbLGr"
      },
      "source": [
        "# Train the model for the first time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjNIOv1OAOKa",
        "outputId": "60836779-31ed-4bd8-effc-d260ac9cf574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2191
        }
      },
      "source": [
        "# Train the model on the data for the first time\n",
        "!python pix2pix-tensorflow/pix2pix.py \\\n",
        "  --mode train \\\n",
        "  --output_dir /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/face2face-model \\\n",
        "  --max_epochs 10000 \\\n",
        "  --input_dir /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/photos/combined/train \\\n",
        "  --which_direction AtoB \\\n",
        "  --display_freq 50 \\\n",
        "  --save_freq 200"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aspect_ratio = 1.0\n",
            "batch_size = 1\n",
            "beta1 = 0.5\n",
            "checkpoint = None\n",
            "display_freq = 50\n",
            "flip = True\n",
            "gan_weight = 1.0\n",
            "input_dir = /content/gdrive/My Drive/DeepLearning/DelCloseBot/photos/combined/train\n",
            "l1_weight = 100.0\n",
            "lab_colorization = False\n",
            "lr = 0.0002\n",
            "max_epochs = 10000\n",
            "max_steps = None\n",
            "mode = train\n",
            "ndf = 64\n",
            "ngf = 64\n",
            "output_dir = /content/gdrive/My Drive/DeepLearning/DelCloseBot/face2face-model\n",
            "output_filetype = png\n",
            "progress_freq = 50\n",
            "save_freq = 200\n",
            "scale_size = 286\n",
            "seed = 230210217\n",
            "separable_conv = False\n",
            "summary_freq = 100\n",
            "trace_freq = 0\n",
            "which_direction = AtoB\n",
            "examples count = 1719\n",
            "2019-01-26 05:22:56.547765: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-26 05:22:56.547825: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-26 05:22:56.547843: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-26 05:22:56.547857: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-26 05:22:56.547880: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
            "parameter_count = 57190084\n",
            "saving display images\n",
            "progress  epoch 1  step 50  image/sec 0.2  remaining 1770555m\n",
            "discrim_loss 0.36462802\n",
            "gen_loss_GAN 0.55249447\n",
            "gen_loss_L1 0.11272525\n",
            "recording summary\n",
            "saving display images\n",
            "progress  epoch 1  step 100  image/sec 0.2  remaining 1773797m\n",
            "discrim_loss 0.54484457\n",
            "gen_loss_GAN 0.944334\n",
            "gen_loss_L1 0.1571037\n",
            "saving display images\n",
            "progress  epoch 1  step 150  image/sec 0.2  remaining 1769572m\n",
            "discrim_loss 0.6251382\n",
            "gen_loss_GAN 1.208584\n",
            "gen_loss_L1 0.17649421\n",
            "recording summary\n",
            "saving display images\n",
            "progress  epoch 1  step 200  image/sec 0.2  remaining 1779631m\n",
            "discrim_loss 0.6283561\n",
            "gen_loss_GAN 1.4216208\n",
            "gen_loss_L1 0.18814583\n",
            "saving model\n",
            "saving display images\n",
            "progress  epoch 1  step 250  image/sec 0.2  remaining 1788627m\n",
            "discrim_loss 0.63871604\n",
            "gen_loss_GAN 1.5246307\n",
            "gen_loss_L1 0.19083282\n",
            "recording summary\n",
            "saving display images\n",
            "progress  epoch 1  step 300  image/sec 0.2  remaining 1790998m\n",
            "discrim_loss 0.6746191\n",
            "gen_loss_GAN 1.5931005\n",
            "gen_loss_L1 0.19014433\n",
            "saving display images\n",
            "progress  epoch 1  step 350  image/sec 0.2  remaining 1790234m\n",
            "discrim_loss 0.72490776\n",
            "gen_loss_GAN 1.5647954\n",
            "gen_loss_L1 0.19419308\n",
            "recording summary\n",
            "saving display images\n",
            "progress  epoch 1  step 400  image/sec 0.2  remaining 1793006m\n",
            "discrim_loss 0.75007766\n",
            "gen_loss_GAN 1.5428147\n",
            "gen_loss_L1 0.18687215\n",
            "saving model\n",
            "saving display images\n",
            "progress  epoch 1  step 450  image/sec 0.2  remaining 1795168m\n",
            "discrim_loss 0.7977119\n",
            "gen_loss_GAN 1.5175495\n",
            "gen_loss_L1 0.18416776\n",
            "recording summary\n",
            "saving display images\n",
            "progress  epoch 1  step 500  image/sec 0.2  remaining 1794385m\n",
            "discrim_loss 0.8339322\n",
            "gen_loss_GAN 1.4739778\n",
            "gen_loss_L1 0.17958654\n",
            "saving display images\n",
            "progress  epoch 1  step 550  image/sec 0.2  remaining 1792226m\n",
            "discrim_loss 0.88997525\n",
            "gen_loss_GAN 1.496081\n",
            "gen_loss_L1 0.17999573\n",
            "recording summary\n",
            "saving display images\n",
            "progress  epoch 1  step 600  image/sec 0.2  remaining 1790758m\n",
            "discrim_loss 0.87414914\n",
            "gen_loss_GAN 1.4553508\n",
            "gen_loss_L1 0.17766306\n",
            "saving model\n",
            "saving display images\n",
            "progress  epoch 1  step 650  image/sec 0.2  remaining 1791282m\n",
            "discrim_loss 0.9027797\n",
            "gen_loss_GAN 1.4127319\n",
            "gen_loss_L1 0.17544234\n",
            "recording summary\n",
            "saving display images\n",
            "progress  epoch 1  step 700  image/sec 0.2  remaining 1791418m\n",
            "discrim_loss 0.92564297\n",
            "gen_loss_GAN 1.385544\n",
            "gen_loss_L1 0.17214023\n",
            "saving display images\n",
            "progress  epoch 1  step 750  image/sec 0.2  remaining 1790581m\n",
            "discrim_loss 0.96472734\n",
            "gen_loss_GAN 1.3374575\n",
            "gen_loss_L1 0.16786088\n",
            "recording summary\n",
            "saving display images\n",
            "progress  epoch 1  step 800  image/sec 0.2  remaining 1790880m\n",
            "discrim_loss 0.9870555\n",
            "gen_loss_GAN 1.3314433\n",
            "gen_loss_L1 0.16536006\n",
            "saving model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ds_r32sbNQF"
      },
      "source": [
        "# Train the model from checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmsGdV5Fl11P",
        "outputId": "c4fbee0b-7b27-4a1f-83c2-05a758104cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1222
        }
      },
      "source": [
        "# Train the model on the data from a checkpoint\n",
        "!python pix2pix-tensorflow/pix2pix.py \\\n",
        "  --mode train \\\n",
        "  --output_dir /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/face2face-model \\\n",
        "  --max_epochs 200 \\\n",
        "  --input_dir /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/photos/combined/train \\\n",
        "  --which_direction AtoB \\\n",
        "  --display_freq 50 \\\n",
        "  --checkpoint /content/gdrive/My\\ Drive/DeepLearning/DelCloseBot/face2face-model \\\n",
        "  --save_freq 200"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aspect_ratio = 1.0\n",
            "batch_size = 1\n",
            "beta1 = 0.5\n",
            "checkpoint = /content/gdrive/My Drive/DeepLearning/DelCloseBot/face2face-model\n",
            "display_freq = 50\n",
            "flip = True\n",
            "gan_weight = 1.0\n",
            "input_dir = /content/gdrive/My Drive/DeepLearning/DelCloseBot/photos/combined/train\n",
            "l1_weight = 100.0\n",
            "lab_colorization = False\n",
            "lr = 0.0002\n",
            "max_epochs = 200\n",
            "max_steps = None\n",
            "mode = train\n",
            "ndf = 64\n",
            "ngf = 64\n",
            "output_dir = /content/gdrive/My Drive/DeepLearning/DelCloseBot/face2face-model\n",
            "output_filetype = png\n",
            "progress_freq = 50\n",
            "save_freq = 200\n",
            "scale_size = 286\n",
            "seed = 699686393\n",
            "separable_conv = False\n",
            "summary_freq = 100\n",
            "trace_freq = 0\n",
            "which_direction = AtoB\n",
            "examples count = 320\n",
            "2019-01-14 03:18:18.719988: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-14 03:18:18.720049: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-14 03:18:18.720067: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-14 03:18:18.720119: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-14 03:18:18.720155: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
            "parameter_count = 57190084\n",
            "loading model from checkpoint\n",
            "saving display images\n",
            "progress  epoch 13  step 10  image/sec 0.2  remaining 6652m\n",
            "discrim_loss 0.8314054\n",
            "gen_loss_GAN 1.7175435\n",
            "gen_loss_L1 0.14422226\n",
            "recording summary\n",
            "saving display images\n",
            "progress  epoch 13  step 60  image/sec 0.2  remaining 6684m\n",
            "discrim_loss 0.7959122\n",
            "gen_loss_GAN 1.7385631\n",
            "gen_loss_L1 0.14079136\n",
            "saving display images\n",
            "progress  epoch 13  step 110  image/sec 0.2  remaining 6661m\n",
            "discrim_loss 0.8159445\n",
            "gen_loss_GAN 1.6695079\n",
            "gen_loss_L1 0.13561772\n",
            "recording summary\n",
            "saving display images\n",
            "progress  epoch 13  step 160  image/sec 0.2  remaining 6662m\n",
            "discrim_loss 0.8240975\n",
            "gen_loss_GAN 1.6374682\n",
            "gen_loss_L1 0.13657467\n",
            "saving model\n",
            "saving display images\n",
            "progress  epoch 13  step 210  image/sec 0.2  remaining 6673m\n",
            "discrim_loss 0.8378862\n",
            "gen_loss_GAN 1.6688325\n",
            "gen_loss_L1 0.14199306\n",
            "recording summary\n",
            "saving display images\n",
            "progress  epoch 13  step 260  image/sec 0.2  remaining 6667m\n",
            "discrim_loss 0.8460852\n",
            "gen_loss_GAN 1.6969657\n",
            "gen_loss_L1 0.14471893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7N_o1MejeqL"
      },
      "source": [
        "# Now we export the model, by reducing and freezing it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkoNCb2P5cmZ",
        "outputId": "534485ad-854d-4937-ac6f-c255066f25e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "#Reduce the model to save space during webcam inference\n",
        "!python face2face-demo/reduce_model.py \\\n",
        "  --model-input gdrive/My\\ Drive/DeepLearning/DelCloseBot/face2face-model \\\n",
        "  --model-output gdrive/My\\ Drive/DeepLearning/DelCloseBot/face2face-model-reduced\n",
        "\n",
        "#Freeze the Reduced Model |saves a file frozen_model.pb, download this or use one in link below|\n",
        "!python face2face-demo/freeze_model.py --model-folder gdrive/My\\ Drive/DeepLearning/DelCloseBot/face2face-model-reduced"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-01-27 22:33:00.045876: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-27 22:33:00.045929: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-27 22:33:00.045946: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-27 22:33:00.045970: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-27 22:33:00.046000: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
            "Model is exported to /content/gdrive/My Drive/DeepLearning/DelCloseBot/face2face-model/model-12800\n",
            "2019-01-27 22:33:32.184736: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-27 22:33:32.184813: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-27 22:33:32.184840: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-27 22:33:32.184868: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2019-01-27 22:33:32.184882: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
            "Converted 60 variables to const ops.\n",
            "759 ops in the final graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeXvxWD-bbFR"
      },
      "source": [
        "# Instructions to run locally with webcam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqEcOXMpj91B"
      },
      "source": [
        "Now run the demo - need to download model and do local\n",
        "\n",
        "download the face predictor model here:\n",
        "http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "download frozen model above or this one:\n",
        "https://drive.google.com/open?id=1DkGbtIGmEdoOB_Ar4e3Tem8j9V7MemGb\n",
        "\n",
        "Save them and modify the locations shown below\n",
        "open terminal\n",
        "\n",
        "Note that you will create a conda environment, with an older version of tensor flow that needs to match the one used above to generate the model, errors can occur if the versions do not match\n",
        "\n",
        "## first time\n",
        "* show 0 to show the live webcam too, \n",
        "* show 2 to show large output image only, \n",
        "* show 1 is face tracking\n",
        "\n",
        "```\n",
        "git clone https://github.com/kylemath/face2face-demo.git\n",
        "cd face2face-demo\n",
        "conda env create -f environment.yml\n",
        "source activate face2face-demo\n",
        "python run_webcam_only.py \\\n",
        "  --source 0 \\\n",
        "  --show 0 \\\n",
        "  --landmark-model /Users/kylemathewson/face2face-demo/shape_predictor_68_face_landmarks.dat \\\n",
        "  --tf-model /Users/kylemathewson/face2face-demo/frozen_model.pb\n",
        "  \n",
        "```\n",
        "## subsequent times\n",
        "* show 0 to show the live webcam too, \n",
        "* show 2 to show large output image only, \n",
        "* show 1 is face tracking\n",
        "\n",
        "```\n",
        "cd face2face-demo\n",
        "source activate face2face-demo\n",
        "python run_webcam_only.py \\\n",
        "  --source 0 \\\n",
        "  --show 2 \\\n",
        "  --landmark-model /Users/kylemathewson/face2face-demo/shape_predictor_68_face_landmarks.dat \\\n",
        "  --tf-model /Users/kylemathewson/face2face-demo/frozen_model.pb\n",
        "  ```\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSLyagSQZjKs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}